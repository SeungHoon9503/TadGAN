# -*- coding: utf-8 -*-
"""TadGAN_Mining data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ieuhrWJBLEUUrYP3SiUPOrRrEW9ITDYc
"""

! pip install orion-ml
! pip install 'urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1'


! git clone https://github.com/signals-dev/Orion.git
! mv Orion/tutorials/tulog/* .

from google.colab import files
files.upload()

from google.colab import files
files.upload()

# general imports 
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
#importing sklearn module
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
#utils.py contains all the plot function.

from utils2 import plot, plot_ts, plot_rws, plot_error, unroll_ts

"""TadGAN에 맞는 데이터 구성 변경"""

signal = pd.read_csv('tadgan_1.csv')

signal.dtypes

signal['date'] = pd.to_datetime(signal['date'])

signal.dtypes

signal.head(5)

signal['ts'] = signal.date.values.astype(np.int64) // 10 ** 9

signal = signal[['ts', 'Silica Concentrate', 'date']]
signal

signal = signal.drop(['date'], axis=1)
signal

signal.rename(columns={'ts':'timestamp'}, inplace=True)

signal.rename(columns={'Silica Concentrate':'value'}, inplace=True)
signal

signal.dtypes

signal

signal['anomaly'] = 0

signal.dtypes

signal

df=signal

df.to_csv("TadGAN.csv", index=False, encoding="utf-8-sig")

signal2 = signal.drop(labels=range(0,148), axis=0)
signal2

signal2 = signal2.reset_index(drop=True)
signal2

signal3= signal2.drop(0, axis=0)
signal3

df2=signal2
df2.to_csv("TadGAN2.csv", index=False, encoding="utf-8-sig")

df3=signal3
df3.to_csv("TadGAN3.csv", index=False, encoding="utf-8-sig")

"""LSTM_Dynamic_Threshold"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    },
    "mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences#1": {
        "window_size": 100
    },
    "keras.Sequential.LSTMTimeSeriesRegressor#1": {
        "epochs": 5,
        "verbose": True
    }
}

orion = Orion(
    pipeline='lstm_dynamic_threshold',
    hyperparameters=hyperparameters
)

anomalies = orion.fit_detect(data)
anomalies.head(5)

"""1차시기"""

plot(data, anomalies)

"""2차시기"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data2 = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    },
    "mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences#1": {
        "window_size": 100
    },
    "keras.Sequential.LSTMTimeSeriesRegressor#1": {
        "epochs": 5,
        "verbose": True
    }
}

orion = Orion(
    pipeline='lstm_dynamic_threshold',
    hyperparameters=hyperparameters
)

anomalies2 = orion.fit_detect(data2)
anomalies2.head(5)

plot(data2, anomalies2)

"""3차시기"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data3 = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    },
    "mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences#1": {
        "window_size": 100
    },
    "keras.Sequential.LSTMTimeSeriesRegressor#1": {
        "epochs": 5,
        "verbose": True
    }
}

orion = Orion(
    pipeline='lstm_dynamic_threshold',
    hyperparameters=hyperparameters
)

anomalies3 = orion.fit_detect(data3)
anomalies3.head(5)

plot(data3, anomalies3)

"""LSTM_Autoencoder"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data11 = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    },
    "mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences#1": {
        "window_size": 100
    },
    "keras.Sequential.LSTMSeq2Seq#1": {
        "epochs": 5,
        "verbose": True
    }
}

orion = Orion(
    pipeline='lstm_autoencoder',
    hyperparameters=hyperparameters
)

anomalies11 = orion.fit_detect(data11)
anomalies11.head(5)

"""1차시기"""

plot(data11, anomalies11)

"""2차시기"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data12 = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    },
    "mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences#1": {
        "window_size": 100
    },
    "keras.Sequential.LSTMSeq2Seq#1": {
        "epochs": 5,
        "verbose": True
    }
}

orion = Orion(
    pipeline='lstm_autoencoder',
    hyperparameters=hyperparameters
)

anomalies12 = orion.fit_detect(data12)
anomalies12.head(5)

plot(data12, anomalies12)

"""3차시기"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data13 = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    },
    "mlprimitives.custom.timeseries_preprocessing.rolling_window_sequences#1": {
        "window_size": 100
    },
    "keras.Sequential.LSTMSeq2Seq#1": {
        "epochs": 5,
        "verbose": True
    }
}

orion = Orion(
    pipeline='lstm_autoencoder',
    hyperparameters=hyperparameters
)

anomalies13 = orion.fit_detect(data13)
anomalies13.head(5)

plot(data13, anomalies13)

"""Tadgan 다시돌리기(2/23)"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data21 = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    }
}

orion = Orion(
    pipeline='tadgan.json',
    hyperparameters=hyperparameters
)

anomalies21 = orion.fit_detect(data21)
anomalies21.head(5)

"""다시 돌린거 plot(1차시기)"""

plot(data21, anomalies21)

"""2차시기"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data22 = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    }
}

orion = Orion(
    pipeline='tadgan.json',
    hyperparameters=hyperparameters
)

anomalies22 = orion.fit_detect(data22)
anomalies22.head(5)

plot(data22, anomalies22)

"""3차시기"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data23 = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    }
}

orion = Orion(
    pipeline='tadgan.json',
    hyperparameters=hyperparameters
)

anomalies23 = orion.fit_detect(data23)
anomalies23.head(5)

plot(data23, anomalies23)

def time_segments_aggregate(X, interval, time_column, method=['mean']):
    """Aggregate values over given time span.
    Args:
        X (ndarray or pandas.DataFrame):
            N-dimensional sequence of values.
        interval (int):
            Integer denoting time span to compute aggregation of.
        time_column (int):
            Column of X that contains time values.
        method (str or list):
            Optional. String describing aggregation method or list of strings describing multiple
            aggregation methods. If not given, `mean` is used.
    Returns:
        ndarray, ndarray:
            * Sequence of aggregated values, one column for each aggregation method.
            * Sequence of index values (first index of each aggregated segment).
    """
    if isinstance(X, np.ndarray):
        X = pd.DataFrame(X)

    X = X.sort_values(time_column).set_index(time_column)

    if isinstance(method, str):
        method = [method]

    start_ts = X.index.values[0]
    max_ts = X.index.values[-1]

    values = list()
    index = list()
    while start_ts <= max_ts:
        end_ts = start_ts + interval
        subset = X.loc[start_ts:end_ts - 1]
        aggregated = [
            getattr(subset, agg)(skipna=True).values
            for agg in method
        ]
        values.append(np.concatenate(aggregated))
        index.append(start_ts)
        start_ts = end_ts

    return np.asarray(values), np.asarray(index)

X, index = time_segments_aggregate(data22, interval=3600, time_column='timestamp')

imp = SimpleImputer()
X = imp.fit_transform(X)

scaler = MinMaxScaler(feature_range=(-1, 1))
X = scaler.fit_transform(X)

plot_ts(X)

def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,
                             drop=None, drop_windows=False):
    """Create rolling window sequences out of time series data.
    The function creates an array of input sequences and an array of target sequences by rolling
    over the input sequence with a specified window.
    Optionally, certain values can be dropped from the sequences.
    Args:
        X (ndarray):
            N-dimensional sequence to iterate over.
        index (ndarray):
            Array containing the index values of X.
        window_size (int):
            Length of the input sequences.
        target_size (int):
            Length of the target sequences.
        step_size (int):
            Indicating the number of steps to move the window forward each round.
        target_column (int):
            Indicating which column of X is the target.
        drop (ndarray or None or str or float or bool):
            Optional. Array of boolean values indicating which values of X are invalid, or value
            indicating which value should be dropped. If not given, `None` is used.
        drop_windows (bool):
            Optional. Indicates whether the dropping functionality should be enabled. If not
            given, `False` is used.
    Returns:
        ndarray, ndarray, ndarray, ndarray:
            * input sequences.
            * target sequences.
            * first index value of each input sequence.
            * first index value of each target sequence.
    """
    out_X = list()
    out_y = list()
    X_index = list()
    y_index = list()
    target = X[:, target_column]

    if drop_windows:
        if hasattr(drop, '__len__') and (not isinstance(drop, str)):
            if len(drop) != len(X):
                raise Exception('Arrays `drop` and `X` must be of the same length.')
        else:
            if isinstance(drop, float) and np.isnan(drop):
                drop = np.isnan(X)
            else:
                drop = X == drop

    start = 0
    max_start = len(X) - window_size - target_size + 1
    while start < max_start:
        end = start + window_size

        if drop_windows:
            drop_window = drop[start:end + target_size]
            to_drop = np.where(drop_window)[0]
            if to_drop.size:
                start += to_drop[-1] + 1
                continue

        out_X.append(X[start:end])
        out_y.append(target[end:end + target_size])
        X_index.append(index[start])
        y_index.append(index[end])
        start = start + step_size

    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)

X, y, X_index, y_index = rolling_window_sequences(X, index, 
                                                  window_size=100, 
                                                  target_size=1, 
                                                  step_size=1,
                                                  target_column=0)

print("Training data input shape: {}".format(X.shape))
print("Training data index shape: {}".format(X_index.shape))
print("Training y shape: {}".format(y.shape))
print("Training y index shape: {}".format(y_index.shape))

plot_rws(X)

from model import hyperparameters
from orion.primitives.tadgan import TadGAN

hyperparameters["epochs"] = 5
hyperparameters["input_shape"] = (100, 1) # based on the window size
hyperparameters["optimizer"] = "keras.optimizers.Adam"
hyperparameters["learning_rate"] = 0.0005
hyperparameters["latent_dim"] = 20
hyperparameters["batch_size"] = 64

tgan = TadGAN(**hyperparameters)
tgan.fit(X,X)

# reconstruct
X_hat, critic = tgan.predict(X,X)

# visualize X_hat
plot_rws(X_hat)

# flatten the predicted windows
y_hat = unroll_ts(X_hat)

# plot the time series
plot_ts([y, y_hat], labels=['original', 'reconstructed'])

# pair-wise error calculation
error = np.zeros(shape=y.shape)
length = y.shape[0]
for i in range(length):
    error[i] = abs(y_hat[i] - y[i])

# visualize the error curve
fig = plt.figure(figsize=(30, 3))
plt.plot(error)
plt.show()

from orion.primitives.tadgan import score_anomalies

error, true_index, true, pred = score_anomalies(X, X_hat, critic, X_index, rec_error_type="dtw", comb="mult")
pred = np.array(pred).mean(axis=2)

# visualize the error curve
plot_error([[true, pred], error])

# threshold
thresh = 4

intervals = list()

i = 0
max_start = len(error)
while i < max_start:
    j = i
    start = index[i]
    while error[i] > thresh:
        i += 1
    
    end = index[i]
    if start != end:
        intervals.append((start, end, np.mean(error[j: i+1])))
        
    i += 1
        
intervals

anomalies221 = pd.DataFrame(intervals, columns=['start', 'end', 'score'])
plot(data22, anomalies221)

from orion.primitives.timeseries_anomalies import find_anomalies

# find anomalies
intervals = find_anomalies(error, index, 
                           window_size_portion=0.21, 
                           window_step_size_portion=0.1, 
                           fixed_threshold=True)
intervals

# visualize the result
anomalies222 = pd.DataFrame(intervals, columns=['start', 'end', 'score'])
plot(data22, anomalies222)

"""<그냥 계산>"""

a = [4.600506495346245, 4.071482249257911, 4.374989111139132, 4.459939659887095,4.063272212967161, 4.617090819205088, 5.004821381084699, 4.980031716338648, 5.295049444342798]
np.mean(a)

np.std(a)

import numpy as np
num_mean = np.mean(error) 
print(num_mean)

num_var = np.var(error) 
print(num_var)

std = np.std(error) 
print(std)

from orion import Orion

parameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
            "interval": 3600 # hour level
        },
    'orion.primitives.tadgan.TadGAN#1': {
        'epochs': 5,
        }
}

orion = Orion(
    'tadgan.json',
    parameters
)

anomalies = orion.fit_detect(data)

plot(data, anomalies)

import numpy as np

# to reproduce the same dummy signal
np.random.seed(0)

# dummy data
start, end = (1, 20)
signal = np.random.rand(end - start, 1)

ground_truth = [
    (5, 8),
    (12, 13),
    (17, 18)
]

anomalies = [
    (5, 8),
    (12, 15)
]

import matplotlib.pyplot as plt

time = range(start, end)
plt.plot(time, signal)

# ground truth
for i, (t1, t2) in enumerate(ground_truth):
    plt.axvspan(t1, t2+1, color="g", alpha=0.2, label="ground_truth")

# detected
for i, (t1, t2) in enumerate(anomalies):
    plt.axvspan(t1, t2+1, color="r", alpha=0.2, label="detected")

    
plt.title("Example")
plt.xlabel("Time")
plt.ylabel("value")
plt.show()

from orion.evaluation.contextual import contextual_accuracy, contextual_f1_score

accuracy = contextual_accuracy(ground_truth, anomalies, start=start, end=end)
f1_score = contextual_f1_score(ground_truth, anomalies, start=start, end=end)

print("Accuracy score = {:0.3f}".format(accuracy))
print("F1 score = {:0.3f}".format(f1_score))

f1_score = contextual_f1_score(ground_truth, anomalies, start=start, end=end, weighted=False)

print("F1 score = {:0.3f}".format(f1_score))

from orion import Orion
from orion.data import load_signal, load_anomalies
from orion.evaluation.contextual import contextual_accuracy, contextual_f1_score, contextual_precision

metrics = [
    'f1',
    'recall',
    'precision',
]

orion = Orion(
    'tadgan.json'
)

signal = 'TadGAN.csv'

# load signal
df = load_signal(signal)

# load ground truth anomalies
ground_truth = load_anomalies(signal)

scores = orion.evaluate(df, ground_truth, fit=True, metrics=metrics)

scores

"""3차시기 TadGAN 코드"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    }
}

orion = Orion(
    pipeline='tadgan.json',
    hyperparameters=hyperparameters
)

anomalies = orion.fit_detect(data)
anomalies.head(5)

"""TadGAN 4차시기"""

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    }
}

orion = Orion(
    pipeline='tadgan.json',
    hyperparameters=hyperparameters
)

anomalies = orion.fit_detect(data)
anomalies.head(5)

filename = "TadGAN.csv"
from orion.data import load_signal, load_anomalies
data = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    }
}

orion = Orion(
    pipeline='tadgan.json',
    hyperparameters=hyperparameters
)

anomalies = orion.fit_detect(data)
anomalies.head(10)

"""Tadgan 1차시기"""

plot(data, anomalies)

"""TadGAN 2차시기"""

plot(data, anomalies)

"""TadGAN 3차시기"""

plot(data, anomalies)

"""TadGAN 4차시기"""

plot(data, anomalies)

"""TadGAN 5차시기"""

plot(data, anomalies)

"""TadGAN2 학습"""

filename = "TadGAN2.csv"
from orion.data import load_signal, load_anomalies
data = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    }
}

orion = Orion(
    pipeline='tadgan.json',
    hyperparameters=hyperparameters
)

anomalies2 = orion.fit_detect(data)
anomalies2.head(10)

plot(data, anomalies)

filename = "TadGAN3.csv"
from orion.data import load_signal, load_anomalies
data = load_signal(filename)

from orion import Orion

hyperparameters = {
    "mlprimitives.custom.timeseries_preprocessing.time_segments_aggregate#1": {
        "interval": 3600
    },
        "orion.primitives.tadgan.TadGAN#1": {
            "epochs": 35,
            "input_shape":[100, 1],
            "target_shape": [100, 1]
        }
}

orion = Orion(
    pipeline='tadgan.json',
    hyperparameters=hyperparameters
)

anomalies3 = orion.fit_detect(data)
anomalies3.head(10)

plot(data, anomalies3)

